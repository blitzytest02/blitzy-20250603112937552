# pytest Configuration for Flask Migration Tutorial Application
#
# This configuration file defines comprehensive testing parameters for the Python Flask
# Migration Tutorial, replacing Jest functionality with pytest-based testing patterns.
# Enforces 100% code coverage requirements and integrates with CI/CD pipelines for
# automated quality validation and educational demonstration of production-grade Python
# testing practices.
#
# Key Features:
# - 100% coverage enforcement across all metrics (line, branch, function, statement)
# - Comprehensive test discovery with Python-specific patterns
# - Multi-format reporting for CI/CD integration and stakeholder visibility
# - Performance validation with pytest-benchmark integration
# - Test categorization through markers for organized execution
# - Flask-specific testing configuration with pytest-flask integration

[tool:pytest]

# ============================================================================
# TEST DISCOVERY AND EXECUTION PATTERNS (Section 6.6.2.1)
# ============================================================================

# Test directory configuration - consistent with Python conventions
testpaths = src/backend/tests

# Test file discovery patterns - replacing Jest discovery with Python patterns
python_files = test_*.py *_test.py

# Test function discovery patterns - standard pytest naming conventions
python_functions = test_*

# Test class discovery patterns - organized test suites
python_classes = Test*

# ============================================================================
# CODE COVERAGE CONFIGURATION (Section 6.6.4.1)
# ============================================================================

# Comprehensive coverage options with 100% enforcement across all metrics
addopts = 
    --strict-markers
    --strict-config
    --verbose
    --tb=short
    --cov=src/backend
    --cov-branch
    --cov-fail-under=100
    --cov-report=term-missing
    --cov-report=html:htmlcov
    --cov-report=xml:coverage.xml
    --cov-report=json:coverage.json
    --cov-context=test
    --no-cov-on-fail
    --cov-config=.coveragerc

# ============================================================================
# TEST EXECUTION CONFIGURATION
# ============================================================================

# Minimum Python version requirement for Flask application
minversion = 8.0

# Required pytest plugins for Flask testing ecosystem
required_plugins = 
    pytest-flask>=1.3.0
    pytest-cov>=5.0.0
    pytest-benchmark>=4.0.0
    pytest-xdist>=3.5.0
    pytest-html>=4.1.1

# Test timeout configuration - prevent hanging tests
timeout = 300
timeout_method = thread

# Console output configuration for enhanced debugging
console_output_style = progress
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s (%(filename)s:%(lineno)d)
log_cli_date_format = %Y-%m-%d %H:%M:%S

# Log file configuration for CI/CD artifact collection
log_file = tests.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s (%(filename)s:%(lineno)d)
log_file_date_format = %Y-%m-%d %H:%M:%S

# ============================================================================
# TEST MARKERS CONFIGURATION (Section 6.6)
# ============================================================================

# Test categorization markers for organized execution and CI/CD pipeline stages
markers =
    unit: Unit tests for individual Flask components and functions
    integration: Integration tests for complete Flask request/response cycles using test client
    performance: Performance tests with pytest-benchmark for SLA validation (<50ms warm, <75MB memory)
    security: Security validation tests for Flask endpoints and configuration
    smoke: Quick smoke tests for basic application functionality verification
    regression: Regression tests for bug fixes and known issue validation
    slow: Tests that take longer than 1 second to execute
    network: Tests requiring network connectivity or external service interaction
    docker: Tests requiring Docker environment for container validation
    benchmark: Performance benchmark tests with statistical analysis
    memory: Memory usage validation tests with psutil monitoring
    load: Load testing scenarios for concurrent request validation
    health: Health check and monitoring endpoint validation tests
    e2e: End-to-end tests covering complete application workflows
    flaky: Tests marked as potentially unstable requiring investigation
    xfail: Expected failure tests during development or investigation
    parametrize: Parametric tests with multiple input scenarios
    fixture: Tests focused on pytest fixture behavior and performance

# ============================================================================
# FLASK-SPECIFIC TESTING CONFIGURATION (Section 6.6.2.2)
# ============================================================================

# Flask application testing environment variables
env =
    FLASK_ENV = testing
    TESTING = 1
    WTF_CSRF_ENABLED = False
    SECRET_KEY = testing-secret-key-not-for-production
    LOG_LEVEL = ERROR

# Disable warnings that are not relevant for testing
filterwarnings =
    ignore::UserWarning
    ignore::DeprecationWarning:distutils
    ignore::PendingDeprecationWarning
    ignore::pytest.PytestUnraisableExceptionWarning
    ignore:.*urllib3.*:DeprecationWarning
    error::pytest.PytestConfigWarning

# ============================================================================
# PARALLEL EXECUTION CONFIGURATION (Section 6.6.3.1.3)
# ============================================================================

# Parallel test execution configuration for performance optimization
# Automatically detect available CPU cores for optimal parallelization
# Usage: pytest -n auto (configured via pytest-xdist)

# ============================================================================
# PERFORMANCE TESTING CONFIGURATION (Section 6.6.11)
# ============================================================================

# pytest-benchmark configuration for performance validation
# Benchmark tests validate Flask application performance against SLA requirements:
# - Cold start: <100ms target
# - Warm request: <50ms target  
# - Memory usage: <75MB limit
# - Concurrent load: <50ms average under 100 parallel requests

# ============================================================================
# HTML REPORTING CONFIGURATION (Section 6.6.3.1.4)
# ============================================================================

# HTML report generation for comprehensive test result visualization
# Reports include test execution summary, coverage integration, performance metrics,
# and error analysis for stakeholder communication and CI/CD integration

# ============================================================================
# CI/CD INTEGRATION SETTINGS (Section 6.6.3.1)
# ============================================================================

# JUnit XML reporting for GitHub Actions test results integration
junit_family = xunit2
junit_logging = all
junit_log_passing_tests = true
junit_duration_report = total

# XML and JSON output configuration for external tool integration
# - coverage.xml for Codecov integration and quality gate enforcement
# - coverage.json for programmatic analysis and trend tracking
# - junit.xml for GitHub Actions test result visualization

# ============================================================================
# DEVELOPMENT AND DEBUGGING CONFIGURATION
# ============================================================================

# Enhanced traceback configuration for efficient debugging
traceback_format = auto
showlocals = true
tb_line_separator = true

# Cache configuration for improved test execution performance
cache_dir = .pytest_cache
collect_ignore = [
    "setup.py",
    "docs",
    "node_modules",
    ".tox",
    ".eggs",
    "*.egg",
    ".git",
    "__pycache__",
    ".coverage",
    "htmlcov",
    "build",
    "dist"
]

# ============================================================================
# VALIDATION AND QUALITY GATES
# ============================================================================

# Enforce strict quality standards through pytest configuration:
# 1. 100% code coverage requirement (--cov-fail-under=100)
# 2. All tests must pass (no tolerance for failures)
# 3. Performance thresholds enforced through benchmark markers
# 4. Security validation through dedicated security markers
# 5. Memory usage validation through psutil integration

# Exit immediately on first failure for fast feedback in development
# Remove or comment out for CI/CD environments requiring complete test execution
# --exitfirst

# Maximum test failures before stopping execution (useful for CI/CD)
# --maxfail=10

# ============================================================================
# FLASK APPLICATION TESTING NOTES
# ============================================================================

# This pytest configuration replaces Jest functionality for the Flask Migration Tutorial:
#
# 1. Test Discovery: Python-specific patterns (test_*.py) replace Jest's automatic JS discovery
# 2. Coverage Enforcement: pytest-cov with 100% requirement replaces Jest coverage thresholds
# 3. Parallel Execution: pytest-xdist provides Jest-like parallel test execution
# 4. Reporting: Multiple format outputs (HTML, XML, JSON) for comprehensive CI/CD integration
# 5. Performance Testing: pytest-benchmark integration for response time and memory validation
# 6. Flask Integration: pytest-flask fixtures provide Flask-specific testing capabilities
# 7. Markers: Comprehensive test categorization for organized execution and CI/CD stages
#
# Educational Value:
# - Demonstrates production-grade Python testing configuration
# - Shows integration between pytest ecosystem tools
# - Provides comprehensive quality validation patterns
# - Illustrates Flask-specific testing best practices
# - Enables scalable test organization through markers
#
# CI/CD Integration:
# - GitHub Actions compatible reporting formats
# - Quality gate enforcement through coverage requirements
# - Performance validation through benchmark integration
# - Security testing integration through marker system
# - Artifact generation for test result visualization